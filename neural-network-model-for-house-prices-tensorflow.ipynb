{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credits to Julien Heiduk: Neural Network Model for House Prices (TensorFlow)\n",
    "# https://www.kaggle.com/zoupet/neural-network-model-for-house-prices-tensorflow\n",
    "# published on https://www.kaggle.com/zoupet/neural-network-model-for-house-prices-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# I. Importation & Devices Available\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"99\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple computation\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.Session()\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])\n",
    "# If you have gpu you can try this line to compute b with your GPU\n",
    "#with tf.device('/gpu:0'):    \n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "print(sess.run(c))\n",
    "\n",
    "# Runs the op.\n",
    "# Log information\n",
    "options = tf.RunOptions(output_partition_graphs=True)\n",
    "metadata = tf.RunMetadata()\n",
    "c_val = sess.run(c, options=options, run_metadata=metadata)\n",
    "\n",
    "print(metadata.partition_graphs)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "CWD_PATH = os.getcwd()\n",
    "TRAIN_PATH = os.path.abspath(os.path.join(CWD_PATH, 'data/house-prices/train.csv'))\n",
    "TEST_PATH = os.path.abspath(os.path.join(CWD_PATH, 'data/house-prices/test.csv'))\n",
    "\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "print('Shape of the train data with all features:\\n', train.shape)\n",
    "train = train.select_dtypes(exclude=['object'])\n",
    "print(\"\")\n",
    "print('Shape of the train data with numerical features:\\n', train.shape)\n",
    "train.drop('Id',axis = 1, inplace = True)\n",
    "train.fillna(0,inplace=True)\n",
    "\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "test = test.select_dtypes(exclude=['object'])\n",
    "ID = test.Id\n",
    "test.fillna(0,inplace=True)\n",
    "test.drop('Id',axis = 1, inplace = True)\n",
    "\n",
    "print(\"\")\n",
    "print(\"List of features contained our dataset:\\n\",list(train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# II. Outliers\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "clf = IsolationForest(max_samples = 100, random_state = 42)\n",
    "clf.fit(train)\n",
    "y_noano = clf.predict(train)\n",
    "y_noano = pd.DataFrame(y_noano, columns = ['Top'])\n",
    "y_noano[y_noano['Top'] == 1].index.values\n",
    "\n",
    "train = train.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "train.reset_index(drop = True, inplace = True)\n",
    "print(\"Number of Outliers:\", y_noano[y_noano['Top'] == -1].shape[0])\n",
    "print(\"Number of rows without outliers:\", train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# III. Preprocessing\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "col_train = list(train.columns)\n",
    "col_train_bis = list(train.columns)\n",
    "\n",
    "col_train_bis.remove('SalePrice')\n",
    "\n",
    "mat_train = np.matrix(train)\n",
    "mat_test  = np.matrix(test)\n",
    "mat_new = np.matrix(train.drop('SalePrice',axis = 1))\n",
    "mat_y = np.array(train.SalePrice).reshape((1314,1))\n",
    "\n",
    "prepro_y = MinMaxScaler()\n",
    "prepro_y.fit(mat_y)\n",
    "\n",
    "prepro = MinMaxScaler()\n",
    "prepro.fit(mat_train)\n",
    "\n",
    "prepro_test = MinMaxScaler()\n",
    "prepro_test.fit(mat_new)\n",
    "\n",
    "train = pd.DataFrame(prepro.transform(mat_train),columns = col_train)\n",
    "test  = pd.DataFrame(prepro_test.transform(mat_test),columns = col_train_bis)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features\n",
    "COLUMNS = col_train\n",
    "FEATURES = col_train_bis\n",
    "LABEL = \"SalePrice\"\n",
    "\n",
    "# Columns for tensorflow\n",
    "feature_cols = [tf.contrib.layers.real_valued_column(k) for k in FEATURES]\n",
    "\n",
    "# Training set and Prediction set with the features to predict\n",
    "training_set = train[COLUMNS]\n",
    "prediction_set = train.SalePrice\n",
    "\n",
    "# Train and Test \n",
    "x_train, x_test, y_train, y_test = train_test_split(training_set[FEATURES] , prediction_set, test_size=0.33, random_state=42)\n",
    "y_train = pd.DataFrame(y_train, columns = [LABEL])\n",
    "training_set = pd.DataFrame(x_train, columns = FEATURES).merge(y_train, left_index = True, right_index = True)\n",
    "training_set.head()\n",
    "\n",
    "# Training for submission\n",
    "training_sub = training_set[col_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing but for the test set\n",
    "y_test = pd.DataFrame(y_test, columns = [LABEL])\n",
    "testing_set = pd.DataFrame(x_test, columns = FEATURES).merge(y_test, left_index = True, right_index = True)\n",
    "testing_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# IV. Deep Neural Network for continuous features\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "regressor = tf.contrib.learn.DNNRegressor(\n",
    "    feature_columns=feature_cols, \n",
    "    activation_fn = tf.nn.relu, hidden_units=[200, 100, 50, 25, 12]\n",
    ")\n",
    "\n",
    "#,#optimizer = tf.train.GradientDescentOptimizer( learning_rate= 0.1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of training\n",
    "training_set.reset_index(drop = True, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(data_set, pred = False):\n",
    "    if pred == False:\n",
    "        feature_cols = {k: tf.constant(data_set[k].values) for k in FEATURES}\n",
    "        labels = tf.constant(data_set[LABEL].values)\n",
    "        return feature_cols, labels\n",
    "    else:\n",
    "        feature_cols = {k: tf.constant(data_set[k].values) for k in FEATURES}\n",
    "        return feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Neural Network Regressor with the training set which contain the data split by train test split\n",
    "regressor.fit(input_fn=lambda: input_fn(training_set), steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on the test set created by train_test_split\n",
    "ev = regressor.evaluate(input_fn=lambda: input_fn(testing_set), steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the score on the testing set\n",
    "# 0.002X in average\n",
    "loss_score1 = ev[\"loss\"]\n",
    "print(\"Final Loss on the testing set: {0:f}\".format(loss_score1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y = regressor.predict(input_fn=lambda: input_fn(testing_set))\n",
    "predictions = list(itertools.islice(y, testing_set.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# V. Predictions and submission\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(prepro_y.inverse_transform(np.array(predictions).reshape(434,1)),columns = ['Prediction'])\n",
    "reality = pd.DataFrame(prepro.inverse_transform(testing_set), columns = [COLUMNS]).SalePrice\n",
    "matplotlib.rc('xtick', labelsize=30) \n",
    "matplotlib.rc('ytick', labelsize=30) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(50, 40))\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.plot(predictions.values, reality.values, 'ro')\n",
    "plt.xlabel('Predictions', fontsize = 30)\n",
    "plt.ylabel('Reality', fontsize = 30)\n",
    "plt.title('Predictions x Reality on dataset Test', fontsize = 30)\n",
    "ax.plot([reality.min(), reality.max()], [reality.min(), reality.max()], 'k--', lw=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = regressor.predict(input_fn=lambda: input_fn(test, pred = True))\n",
    "\n",
    "def to_submit(pred_y,name_out):\n",
    "    y_predict = list(itertools.islice(pred_y, test.shape[0]))\n",
    "    y_predict = pd.DataFrame(prepro_y.inverse_transform(np.array(y_predict).reshape(len(y_predict),1)), columns = ['SalePrice'])\n",
    "    y_predict = y_predict.join(ID)\n",
    "    y_predict.to_csv(name_out + '.csv',index=False)\n",
    "    \n",
    "to_submit(y_predict, \"house_prices_submission_continuous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# VI. Leaky Relu\n",
    "############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x):\n",
    "    return tf.nn.relu(x) - 0.01 * tf.nn.relu(-x)\n",
    "\n",
    "# Model\n",
    "regressor = tf.contrib.learn.DNNRegressor(\n",
    "    feature_columns=feature_cols,\n",
    "    activation_fn = leaky_relu, \n",
    "    hidden_units=[200, 100, 50, 25, 12]\n",
    ")\n",
    "    \n",
    "# Deep Neural Network Regressor with the training set which contain the data split by train test split\n",
    "regressor.fit(input_fn=lambda: input_fn(training_set), steps=2000)\n",
    "# Evaluation on the test set created by train_test_split\n",
    "ev = regressor.evaluate(input_fn=lambda: input_fn(testing_set), steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the score on the testing set\n",
    "# 0.002X in average\n",
    "loss_score2 = ev[\"loss\"]\n",
    "print(\"Final Loss on the testing set with Leaky Relu: {0:f}\".format(loss_score2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_predict = regressor.predict(input_fn=lambda: input_fn(test, pred = True))\n",
    "to_submit(y_predict, \"house_prices_Leaky_relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "regressor = tf.contrib.learn.DNNRegressor(\n",
    "    feature_columns=feature_cols,\n",
    "    activation_fn = tf.nn.elu, \n",
    "    hidden_units=[200, 100, 50, 25, 12]\n",
    ")\n",
    "    \n",
    "# Deep Neural Network Regressor with the training set which contain the data split by train test split\n",
    "regressor.fit(input_fn=lambda: input_fn(training_set), steps=2000)\n",
    "# Evaluation on the test set created by train_test_split\n",
    "ev = regressor.evaluate(input_fn=lambda: input_fn(testing_set), steps=1)\n",
    "\n",
    "loss_score3 = ev[\"loss\"]\n",
    "print(\"Final Loss on the testing set with Elu: {0:f}\".format(loss_score3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_predict = regressor.predict(input_fn=lambda: input_fn(test, pred = True))\n",
    "to_submit(y_predict, \"house_prices_Elu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "# VII. Deep Neural Network for continuous and categorical features\n",
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and split\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "train.drop('Id',axis = 1, inplace = True)\n",
    "train_numerical = train.select_dtypes(exclude=['object'])\n",
    "train_numerical.fillna(0, inplace = True)\n",
    "train_categoric = train.select_dtypes(include=['object'])\n",
    "train_categoric.fillna('NONE', inplace = True)\n",
    "train = train_numerical.merge(train_categoric, left_index = True, right_index = True) \n",
    "\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "ID = test.Id\n",
    "test.drop('Id',axis = 1, inplace = True)\n",
    "test_numerical = test.select_dtypes(exclude=['object'])\n",
    "test_numerical.fillna(0, inplace = True)\n",
    "test_categoric = test.select_dtypes(include=['object'])\n",
    "test_categoric.fillna('NONE', inplace = True)\n",
    "test = test_numerical.merge(test_categoric, left_index = True, right_index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removie the outliers\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "clf = IsolationForest(max_samples = 100, random_state = 42)\n",
    "clf.fit(train_numerical)\n",
    "y_noano = clf.predict(train_numerical)\n",
    "y_noano = pd.DataFrame(y_noano, columns = ['Top'])\n",
    "y_noano[y_noano['Top'] == 1].index.values\n",
    "\n",
    "train_numerical = train_numerical.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "train_numerical.reset_index(drop = True, inplace = True)\n",
    "\n",
    "train_categoric = train_categoric.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "train_categoric.reset_index(drop = True, inplace = True)\n",
    "\n",
    "train = train.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "train.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_train_num = list(train_numerical.columns)\n",
    "col_train_num_bis = list(train_numerical.columns)\n",
    "\n",
    "col_train_cat = list(train_categoric.columns)\n",
    "\n",
    "col_train_num_bis.remove('SalePrice')\n",
    "\n",
    "mat_train = np.matrix(train_numerical)\n",
    "mat_test  = np.matrix(test_numerical)\n",
    "mat_new = np.matrix(train_numerical.drop('SalePrice',axis = 1))\n",
    "mat_y = np.array(train.SalePrice)\n",
    "\n",
    "prepro_y = MinMaxScaler()\n",
    "prepro_y.fit(mat_y.reshape(1314,1))\n",
    "\n",
    "prepro = MinMaxScaler()\n",
    "prepro.fit(mat_train)\n",
    "\n",
    "prepro_test = MinMaxScaler()\n",
    "prepro_test.fit(mat_new)\n",
    "\n",
    "train_num_scale = pd.DataFrame(prepro.transform(mat_train),columns = col_train)\n",
    "test_num_scale  = pd.DataFrame(prepro_test.transform(mat_test),columns = col_train_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[col_train_num] = pd.DataFrame(prepro.transform(mat_train),columns = col_train_num)\n",
    "test[col_train_num_bis]  = test_num_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features\n",
    "COLUMNS = col_train_num\n",
    "FEATURES = col_train_num_bis\n",
    "LABEL = \"SalePrice\"\n",
    "\n",
    "FEATURES_CAT = col_train_cat\n",
    "\n",
    "engineered_features = []\n",
    "\n",
    "for continuous_feature in FEATURES:\n",
    "    engineered_features.append(\n",
    "        tf.contrib.layers.real_valued_column(continuous_feature)\n",
    "    )\n",
    "\n",
    "for categorical_feature in FEATURES_CAT:\n",
    "    sparse_column = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "        categorical_feature, \n",
    "        hash_bucket_size=1000\n",
    "    )\n",
    "    engineered_features.append(tf.contrib.layers.embedding_column(sparse_id_column=sparse_column, dimension=16,combiner=\"sum\"))\n",
    "                                 \n",
    "# Training set and Prediction set with the features to predict\n",
    "training_set = train[FEATURES + FEATURES_CAT]\n",
    "prediction_set = train.SalePrice\n",
    "\n",
    "# Train and Test \n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    training_set[FEATURES + FEATURES_CAT],\n",
    "    prediction_set, test_size=0.33, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "y_train = pd.DataFrame(y_train, columns = [LABEL])\n",
    "training_set = pd.DataFrame(x_train, columns = FEATURES + FEATURES_CAT).merge(y_train, left_index = True, right_index = True)\n",
    "\n",
    "# Training for submission\n",
    "training_sub = training_set[FEATURES + FEATURES_CAT]\n",
    "testing_sub = test[FEATURES + FEATURES_CAT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing but for the test set\n",
    "y_test = pd.DataFrame(y_test, columns = [LABEL])\n",
    "testing_set = pd.DataFrame(x_test, columns = FEATURES + FEATURES_CAT).merge(y_test, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[FEATURES_CAT] = training_set[FEATURES_CAT].applymap(str)\n",
    "testing_set[FEATURES_CAT] = testing_set[FEATURES_CAT].applymap(str)\n",
    "\n",
    "def input_fn_new(data_set, training = True):\n",
    "    continuous_cols = {\n",
    "        k: tf.constant(data_set[k].values) for k in FEATURES\n",
    "    }\n",
    "    \n",
    "    categorical_cols = {\n",
    "        k: tf.SparseTensor(\n",
    "            indices=[[i, 0] for i in range(data_set[k].size)], \n",
    "            values = data_set[k].values, \n",
    "            dense_shape = [data_set[k].size, 1]) for k in FEATURES_CAT\n",
    "    }\n",
    "\n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = dict(list(continuous_cols.items()) + list(categorical_cols.items()))\n",
    "    \n",
    "    if training == True:\n",
    "        # Converts the label column into a constant Tensor.\n",
    "        label = tf.constant(data_set[LABEL].values)\n",
    "        # Returns the feature columns and the label.\n",
    "        return feature_cols, label\n",
    "    return feature_cols\n",
    "\n",
    "# Model\n",
    "regressor = tf.contrib.learn.DNNRegressor(\n",
    "    feature_columns = engineered_features,\n",
    "    activation_fn = tf.nn.relu, \n",
    "    hidden_units=[200, 100, 50, 25, 12]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = {\n",
    "    k: tf.SparseTensor(\n",
    "        indices=[[i, 0] for i in range(training_set[k].size)], \n",
    "        values = training_set[k].values, \n",
    "        dense_shape = [training_set[k].size, 1]) for k in FEATURES_CAT\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Neural Network Regressor with the training set which contain the data split by train test split\n",
    "regressor.fit(input_fn = lambda: input_fn_new(training_set) , steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = regressor.evaluate(input_fn=lambda: input_fn_new(testing_set, training = True), steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_score4 = ev[\"loss\"]\n",
    "print(\"Final Loss on the testing set: {0:f}\".format(loss_score4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# VIII. Predictions bis\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y = regressor.predict(input_fn=lambda: input_fn_new(testing_set))\n",
    "predictions = list(itertools.islice(y, testing_set.shape[0]))\n",
    "predictions = pd.DataFrame(prepro_y.inverse_transform(np.array(predictions).reshape(434,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rc('xtick', labelsize=30) \n",
    "matplotlib.rc('ytick', labelsize=30) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(50, 40))\n",
    "plt.style.use('ggplot')\n",
    "plt.plot(predictions.values, reality.values, 'ro')\n",
    "plt.xlabel('Predictions', fontsize = 30)\n",
    "plt.ylabel('Reality', fontsize = 30)\n",
    "plt.title('Predictions x Reality on dataset Test', fontsize = 30)\n",
    "ax.plot([reality.min(), reality.max()], [reality.min(), reality.max()], 'k--', lw=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = regressor.predict(input_fn=lambda: input_fn_new(testing_sub, training = False))\n",
    "to_submit(y_predict, \"house_prices_submission_cont_categ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# IX. Shallow Network\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "regressor = tf.contrib.learn.DNNRegressor(\n",
    "    feature_columns = engineered_features,\n",
    "    activation_fn = tf.nn.relu, \n",
    "    hidden_units=[1000]\n",
    ")\n",
    "\n",
    "# Deep Neural Network Regressor with the training set which contain the data split by train test split\n",
    "regressor.fit(input_fn = lambda: input_fn_new(training_set) , steps=2000)\n",
    "ev = regressor.evaluate(input_fn=lambda: input_fn_new(testing_set, training = True), steps=1)\n",
    "loss_score5 = ev[\"loss\"]\n",
    "print(\"Final Loss on the testing set: {0:f}\".format(loss_score5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = regressor.predict(input_fn=lambda: input_fn_new(testing_sub, training = False))    \n",
    "to_submit(y_predict, \"house_prices_submission_shallow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X. Conclusion\n",
    "\n",
    "list_score = [loss_score1, loss_score2, loss_score3, loss_score4,loss_score5]\n",
    "list_model = ['Relu_cont', 'LRelu_cont', 'Elu_cont', 'Relu_cont_categ','Shallow_1ku']\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "objects = list_model\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = list_score\n",
    " \n",
    "plt.barh(y_pos, performance, align='center', alpha=0.9)\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Loss ')\n",
    "plt.title('Model compared without hypertuning')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
